## 使用SFT改进中小规模结构化数据的咨询
## 背景介绍
在日常生活中，有许多场景是需要根据已有的结构化的信息、例如Excel/CSV 表格、MySQL数据库和HTML Table等进行咨询和问询，现在企业将LLM融入自身业务中的一个落脚点是希望LLM能在这样的场景中替代人工查询或者复杂的网页填表和选择框，直接通过对话的方式进行语义检索。目前对于这种任务的处理主要使用的商业落地方案集中于使用基于Text-Embedding的RAG(Retrieval-augmented generation, RAG)，但RAG方案在大规模数据和策略设计得当的情况下表现尚可，在中小规模数据中的表现不尽人意，最近自己尝试采用了SFT的方案来解决这个问题，感觉获得了一些比RAG方案更好的结果，将解决思路分享，希望可以启发思考共同进步。
## RAG
RAG技术的基本原理是基于text-embedding的retrival和rerank，它的前身其实是NLP任务中的文本分类任务和聚类、排序等场景下基于text-embedding进行语义检索的一种分支方法，最先开始应用其实并不是在大模型出现之后，早年电商的检索业务：1）图像/视频/多模态检索；2）智能问答；3）推荐/广告等其实都是基于它。Milvus / Qdrant / Chroma 等支持向量索引类型的数据库核心也是实现一个高效的新的语义检索的字段，让用户可以高效的通过类似传统关系型数据库中的一个字段一样，去建立一个与传统基于字符串的 **模糊匹配/正则匹配/规则查找** 方式所不同的**基于Embedding的语义查找**字段。听上去是挺不错的，RAG方案在数据规模充足（上亿）的情况下体现出了不错的粗看效果，Milvus的官网中展示的合作伙伴包括一些著名的大厂，例如 Nvidia / Ebay / Shopee / Line 等，这些大厂并不缺语料来源，但在中小规模（低于1w）的量级下，我自总结了RAG体现出的两大主要缺点：

1. Retrival阶段的准确性十分依赖用户 Query 对于 向量型数据库中的 Corpus 字段的命中率 
2. 十分依赖RAG策略设计 / 语料切分策略 / RAG基于的text-embedding模型的质量

最近Arxiv中也有文章总结了对应 [RAG的七宗罪](https://arxiv.org/abs/2401.05856)，知乎 @西红柿牛腩 也对应的写了一篇文章  [RAG系统的“七宗罪”--一周出demo，半年用不好](https://zhuanlan.zhihu.com/p/677691070)，有兴趣可以详细参考。分析一下为什么在大厂的情况下，这两点为什么并不会成为主要阻碍。在大厂的情况中，数据的丰富度远高于普通的小规模表格内容咨询，一个比较典型的案例是Apple官方对于产品的描述和基础功能的使用文档，官方对于功能文档本身维护相对较好，足够详细且足够多，使得用户Query对于Corpus之间的命中率无形之中就高了起来，它们本身就是很好的RAG语料，不需要复杂的策略设计，直接通过一个支持长上下文的Embedding模型就能实现一个能看的BaseLine结果。另一方面，有部分的Apple用户在进行Query时，本身对于产品是有一定的了解的，例如一个用户问: "**Siri**应该如何设置 **捷径**"，在这个问题里，用户至少知道了Apple的产品中包含**Siri** 这个语音助手和 **捷径** 这个功能，但是只是不知道具体的设置步骤或者方法，但实际上这两条信息已经可以通过Text-Embedding的方式对Corpus语料实现较高的命中率了；并且用户在使用iPhone或者Apple的其他产品时，会在长时间的使用过程中积累这些辅助RAG的对应信息。

而在中小型企业内部，例如，我们需要实现的是对于一张大的Excel表格，针对两万行该企业对应服务或微服务的对应描述，一方面，语料本身并不丰富；另一方面，很有可能咨询场景的用户也对这张表格的专业术语并不熟悉，这使得**Query**对于语料库**Corpus**基于语义的命中率进一步的低了。

![alt text](image.png)
## Why SFT  
SFT并不是一个难以想到的方案，如果我们把RAG比开卷考试，那么SFT对应的就是提前将知识教授给模型，这样的话模型在回答问题时对 **知识** 的搜索就不依赖于查找 **开卷资料** 的算法的准确度，在这种方案下，我们利用了LLM本身的泛化能力，即在非过拟合的情况下，模型可以从SFT阶段接受的知识进行语义上的推断来回答问题。简单思考一下这种情况下我们需要的东西：

1. 基于表格内容信息的SFT语料库问答对
2. 一个用于SFT的开源模型

关于第一点，我们虽然有的只是一张大的关于服务场景的描述表格，但我们可以用较强的**LLM（例如 gpt-4o）模拟一些用户常问的问题**，简单的prompt工程即可实现这件事，另一个好处是这 **并不需要大量的数据**。SFT阶段事实上我们只需要关于某一行的一到两个详细描述，例如：

    Q1：P24这一类的电器产品在做检测测试时，有哪些要点？
    A1：请注意该产品的电压要求是稳定的交流260伏的220Hz交流电压，并且在进行测试时检测人员需要佩戴f20类橡胶手套。
    
    Q2：有哪些产品和它是类似的？
    A2：同样类似的还有F27和Q35类的产品。
    

Q1 问题和对应的回答，完全是可以通过较强的LLM本身进行生成的，你可能会问，那么它和 RAG 之前又有什么区别？我完全可以将 **产品名称** 作为RAG的索引列，当我的问题包含 **P24 关键字** 时，它也能给到 **Q1准确的答案**。完全正确，但是问题在于如果是以 **产品名称** 为索引列时，**P24 / F27 / Q35**三个字段在text-embedding的情况下**Product** 或者**余弦相似度**可能并不高，模型可能会在RAG增强的情况下将P24的要点回答的完整，但是对于 **F27 / Q35** 则很有可能在 RAG 索引的过程中获得并不及其他语料的相关分数，从而被忽略，这时候好的模型会回答：“因为缺乏辅助资料，我并没有看到跟P24相似的产品”，坏的模型已经开始乱编了。

你肯定可以通过一些聪明的RAG策略设计来避免这一点，例如将**A1**的回答也作为RAG语料再检索一次，甚至也提前通过Prompt工程生成这一类的总结不同产品之间的相似性的QA问答作为RAG语料，但是应该承认，这是大部分基于RAG的方案之所以不理想的主要源头所在。

那么SFT会不会出现同样的问题呢？我们是否也同样需要提前为模型添加很多预置知识呢。答案是**并不那么需要**，即使我们没有刻意的 **强调不同产品的相似性**，LLM也可以在对于单个数据的学习过程中认识到不同的样品之间的相似性，这其实是**泛化/ 涌现能力**的体现，也是LLM神奇的地方所在，我会在我自己的例子中展示这一点。

如果你希望基于自己的数据进行标注，推荐使用我写的一个小工具 `sft-data-generator`，它可以根据一个给出的csv/ excel的table文件和`prompt`基于每一行数据进行标注，并将结果进行保存：
```bash
pip install sft-data-generator
```
安装之后，使用样例
```bash
sft-data-generator --file_path table.csv --prompt 请模仿一个测试制样人员，询问关于这个产品的制样细则。 --model <model_name> [other optional arguments]
```